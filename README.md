# ğŸ’¬ RAG Chatbot with Streamlit + LangChain + Google Gemini

A **Retrieval-Augmented Generation (RAG) chatbot** built with **Streamlit**, **LangChain**, and the **Google Gemini API**.  
This chatbot lets you **upload and query your PDF knowledge base**, retrieving **context-aware answers** in a smooth chat-like interface.

---

## ğŸš€ Features

- ğŸ“„ **PDF Knowledge Base** â†’ Chat directly with your documents  
- ğŸ” **Smart Retrieval** â†’ Uses **Chroma DB** for persistent vector storage  
- ğŸ§  **AI-Powered Responses** â†’ Powered by **Google Gemini LLM**  
- ğŸ§© **Semantic Chunking** â†’ Uses `SemanticChunker` for meaningful document splitting  
- ğŸ’¬ **Real-time Chat** â†’ User messages on the right, bot responses on the left  
- â³ **Streaming Mode** â†’ Word-by-word typing effect for a natural chatbot feel  
- ğŸ“Š **Live Statistics** â†’ Track total messages & questions in sidebar  
- ğŸ—‘ï¸ **Clear Chat** â†’ Reset conversation anytime  

---

## ğŸ“‚ Project Structure

**rag-chatbot/**  
**â”‚â”€â”€ main.py** â€“ Streamlit UI and chatbot logic  
**â”‚â”€â”€ vector_store.py** â€“ Creates/loads Chroma vector DB  
**â”‚â”€â”€ splitter.py** â€“ Splits PDFs into semantic chunks  
**â”‚â”€â”€ loader.py** â€“ Loads PDF documents  
**â”‚â”€â”€ config.py** â€“ API keys, model setup, DB paths  
**â”‚â”€â”€ AppleData-2024.pdf** â€“ Sample knowledge base PDF  
**â”‚â”€â”€ requirements.txt** â€“ Python dependencies  
**â”‚â”€â”€ README.md** â€“ Project documentation  
**â”‚â”€â”€ .env** â€“ Store Google API key here  


---

## ğŸ› ï¸ Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/Gurkaran017/RAG-Chatbot.git
   cd rag-chatbot

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt

3. **Setup Environment Variables**
   ```bash
   GOOGLE_API_KEY=your_google_api_key_here

---

## âš™ï¸ How It Works (Architecture)

### ğŸ”¹ Step 1: Document Loading
- Uses `PyPDFLoader` from `langchain_community`  
- Loads all text from **AppleData-2024.pdf**

### ğŸ”¹ Step 2: Semantic Chunking
- Uses **SemanticChunker** with **Google Embeddings**  
- Breaks the document into **meaningful sections** instead of fixed-size chunks

### ğŸ”¹ Step 3: Vector Database (ChromaDB)
- Stores embeddings in **ChromaDB (persistent)**  
- Automatically reuses existing DB if available

### ğŸ”¹ Step 4: Query Retrieval
- User query â†’ Expanded into multiple queries using **MultiQueryRetriever**  
- Fetches **top k=3** most relevant chunks from Chroma

### ğŸ”¹ Step 5: Prompt + LLM
- Builds a **prompt template** with:  
  - Conversation history  
  - Retrieved context  
  - Latest question  
- Sends it to **Google Gemini (`gemini-1.5-flash`)**

### ğŸ”¹ Step 6: Streaming Response
- LLM response streamed **word-by-word**  
- Typing effect for a **natural chat experience**


---

## ğŸ“Š Example Workflow

### 1. Start the App
- Loads **PDF** + **Chroma DB**

### 2. Ask a Question
- Example: *â€œSummarize key pointsâ€*

### 3. Retrieve Relevant Chunks
- Retriever fetches **top matching sections** from the document

### 4. Generate Response
- **Gemini LLM** creates a contextual answer

### 5. Display Answer
- Chat interface shows the response with a **typing animation**

---

## ğŸ“· Screenshots  

### 1. Welcome Page  
![Welcome Page](screenshots/welcome.png)  

### 2. User Asking Question  
![User Asking Question](screenshots/user_question.png)  

### 3. Bot Responding with Typing Effect  
![Bot Response](screenshots/bot_response.png)  


---

## ğŸ“Œ Dependencies

### ğŸ Python
- Python **3.9+**

### ğŸ“¦ Core Libraries
- **Streamlit**  
- **LangChain**  
- **LangChain-Community**  
- **LangChain-Chroma**

### ğŸ—„ï¸ Optional
- **FAISS** â†’ optional backup vector database

### ğŸ¤– AI Model
- **Google Generative AI (Gemini)**

3. **Install via:**
   ```bash
   pip install -r requirements.txt

---

